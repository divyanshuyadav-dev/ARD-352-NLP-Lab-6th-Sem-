{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1382c2-bbd6-47a4-a931-1b0545eec258",
   "metadata": {},
   "source": [
    "# Experiment: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df80e5-64b6-4bf3-a729-fa05ffbfb336",
   "metadata": {},
   "source": [
    "## a. Part of Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428abb61-6cef-4843-b5d3-6d3aa09826da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"The curious cat quietly watched the small bird hop across the garden fence.\"\n",
    "\n",
    "# tag-code defnitions for averaged_preceptron_tagger for clean output print\n",
    "penn_treebank_tags = {\n",
    "    'CC': 'Coordinating conjunction', 'CD': 'Cardinal number', 'DT': 'Determiner', 'EX': 'Existential there',\n",
    "    'FW': 'Foreign word', 'IN': 'Preposition or subordinating conjunction', 'JJ': 'Adjective', 'JJR': 'Adjective, comparative',\n",
    "    'JJS': 'Adjective, superlative', 'LS': 'List item marker', 'MD': 'Modal', 'NN': 'Noun, singular or mass',\n",
    "    'NNS': 'Noun, plural', 'NNP': 'Proper noun, singular', 'NNPS': 'Proper noun, plural', 'PDT': 'Predeterminer',\n",
    "    'POS': 'Possessive ending', 'PRP': 'Personal pronoun', 'PRP$': 'Possessive pronoun', 'RB': 'Adverb',\n",
    "    'RBR': 'Adverb, comparative', 'RBS': 'Adverb, superlative', 'RP': 'Particle', 'SYM': 'Symbol', 'TO': 'to',\n",
    "    'UH': 'Interjection', 'VB': 'Verb, base form', 'VBD': 'Verb, past tense', 'VBG': 'Verb, gerund or present participle',\n",
    "    'VBN': 'Verb, past participle', 'VBP': 'Verb, non-3rd person singular present', 'VBZ': 'Verb, 3rd person singular present',\n",
    "    'WDT': 'Wh-determiner', 'WP': 'Wh-pronoun', 'WP$': 'Possessive wh-pronoun', 'WRB': 'Wh-adverb',\n",
    "    '.': 'Punctuation mark, sentence closer', ',': 'Punctuation mark, comma', ':': 'Punctuation mark, colon or ellipsis',\n",
    "    '(': 'Punctuation mark, opening parenthesis', ')': 'Punctuation mark, closing parenthesis', '\"': 'Quotation mark',\n",
    "    \"''\": 'Closing quotation mark', \"``\": 'Opening quotation mark', '#': 'Symbol, number sign', '$': 'Symbol, dollar sign',\n",
    "}\n",
    "\n",
    "# dinctonary for user-defined mannual tagging\n",
    "tagger_dict = {\n",
    "    \"Determinant\":{\"the\",\"an\", \"a\"},\n",
    "    \"Adjective\":{\"curious\", \"wise\", \"small\"},\n",
    "    \"Noun\":{\"cat\", \"bird\", \"graden\", \"fence\"},\n",
    "    \"Adverb\":{\"quietly\", \"fastly\"},\n",
    "    \"Verb\":{\"watched\", \"hop\"},\n",
    "    \"Preposition\":{\"across\",\"on\"},\n",
    "    \"Punctuation\":{\".\", \",\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb67306-544c-4101-949a-0c6bbc584bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/div/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/div/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:The curious cat quietly watched the small bird hop across the garden fence.\n",
      "\n",
      "Words   POS  Meaning\n",
      "====================\n",
      "The     DT   Determiner\n",
      "curious JJ   Adjective\n",
      "cat     NN   Noun, singular or mass\n",
      "quietly RB   Adverb\n",
      "watched VBD  Verb, past tense\n",
      "the     DT   Determiner\n",
      "small   JJ   Adjective\n",
      "bird    NN   Noun, singular or mass\n",
      "hop     NN   Noun, singular or mass\n",
      "across  IN   Preposition or subordinating conjunction\n",
      "the     DT   Determiner\n",
      "garden  NN   Noun, singular or mass\n",
      "fence   NN   Noun, singular or mass\n",
      ".       .    Punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# Predefined Library: PerceptronTagger\n",
    "import nltk\n",
    "\n",
    "\n",
    "# TOKENIZATION\n",
    "# download the package\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# import the tokeniser function\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tokenize import PunktTokenizer\n",
    "\n",
    "# finally tokenize the text\n",
    "tokens = word_tokenize(text1)\n",
    "\n",
    "\n",
    "# POS TAGGING\n",
    "# Note: dont do tagging after stemming, stemming might remove tagginf hint to tagger model\n",
    "# download the package\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# imports\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "# instantiate and tag\n",
    "tagged = nltk.pos_tag(tokens)#PerceptronTagger().tag(tokens)\n",
    "# print output\n",
    "print(f\"Original Text:{text1}\\n\")\n",
    "print(f\"{\"Words\":8s}{\"POS\":5s}{\"Meaning\"}\\n{'='*20}\")\n",
    "for tag in tagged:\n",
    "    print(f\"{tag[0]:8s}{tag[1]:5s}{penn_treebank_tags[tag[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417a8991-3bc2-445c-91fa-2ccfe101c79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words   POS\n",
      "====================\n",
      "The     Determinant\n",
      "curious Adjective\n",
      "cat     Noun\n",
      "quietly Adverb\n",
      "watched Verb\n",
      "the     Determinant\n",
      "small   Adjective\n",
      "bird    Noun\n",
      "hop     Verb\n",
      "across  Preposition\n",
      "the     Determinant\n",
      "garden  None\n",
      "fence   Noun\n",
      ".       Punctuation\n"
     ]
    }
   ],
   "source": [
    "# User defined methods for POS Tagging\n",
    "def get_tag(token):\n",
    "    for pos in tagger_dict.keys():\n",
    "        if token.lower() in tagger_dict[pos]:\n",
    "            return pos\n",
    "\n",
    "# Use tokenised text from above cell\n",
    "print(f\"{\"Words\":8s}{\"POS\"}\\n{\"=\"*20}\")\n",
    "for token in tokens:\n",
    "    print(f\"{token:8s}{get_tag(token)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df26af7-136f-4a71-82ad-e539d469ec23",
   "metadata": {},
   "source": [
    "## b. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0293cd36-4559-4394-a8fe-ced98be7d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"The researchers were analyzing datasets and discovered that the trained models performed better on cleaned data.\"\n",
    "\n",
    "# dictionary & function to help classify pos tag\n",
    "pos_dic = {\n",
    "    \"a\":{\"better\"}, #adjective\n",
    "    \"v\":{\"were\", \"analyzing\", \"discovered\", \"trained\", \"performed\", \"cleaned\"}, #verb\n",
    "}\n",
    "\n",
    "def get_pos_tag(token):\n",
    "    for pos in pos_dic.keys():\n",
    "        if token in pos_dic[pos]:\n",
    "            return pos\n",
    "    return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2740b0c-c2e4-4b18-a30d-451f0691a93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/div/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The researchers were analyzing datasets and discovered that the trained models performed better on cleaned data.\n",
      "Word         Lemma\n",
      "====================\n",
      "The          The\n",
      "researchers  researcher\n",
      "were         be\n",
      "analyzing    analyze\n",
      "datasets     datasets\n",
      "and          and\n",
      "discovered   discover\n",
      "that         that\n",
      "the          the\n",
      "trained      train\n",
      "models       model\n",
      "performed    perform\n",
      "better       good\n",
      "on           on\n",
      "cleaned      clean\n",
      "data         data\n",
      ".            .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# TOKENIZATION\n",
    "\n",
    "# download package\n",
    "nltk.download(\"punkt_tab\")\n",
    "# import the tokenizer function\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# finally tokenize the text\n",
    "tokens = word_tokenize(text2)\n",
    "\n",
    "# LEMMATIZATION\n",
    "# imports\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# instantiate lemmatizer and lemmatize the tokens\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized = [wnl.lemmatize(token, pos=get_pos_tag(token.lower())) for token in tokens]\n",
    "print(f\"Original Text: {text2}\")\n",
    "print(f\"{\"Word\":13s}{\"Lemma\"}\\n{\"=\"*20}\")\n",
    "\n",
    "for word, lemma in zip(tokens, lemmatized):\n",
    "    print(f\"{word:13s}{lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf81a5-9ac7-4a96-9aea-d5b04d04f43b",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784bc9a-5831-45c4-8a71-0c5866380e8e",
   "metadata": {},
   "source": [
    "### 1. Study and use the Stanford Part of speech tagger on a suitable corpus available freely. The corpus should be of decent size. (Use spaCy and stanza)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e6a622-16be-414b-a556-c6496f8c2f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/div/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('brown')\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Join sentences into text (use first 20k sentences for decent size)\n",
    "sentences = brown.sents()[:2]\n",
    "text = [\" \".join(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2287cdca-772a-4067-acdc-1afa3a4eaa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 439kB [00:00, 20.1MB/s]                                                                 \n",
      "2026-02-21 14:56:02 INFO: Downloaded file to /home/div/stanza_resources/resources.json\n",
      "2026-02-21 14:56:02 INFO: Downloading default packages for language: en (English) ...\n",
      "2026-02-21 14:56:04 INFO: File exists: /home/div/stanza_resources/en/default.zip\n",
      "2026-02-21 14:56:14 INFO: Finished downloading models and saved to /home/div/stanza_resources\n",
      "2026-02-21 14:56:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 439kB [00:00, 14.5MB/s]                                                                 \n",
      "2026-02-21 14:56:14 INFO: Downloaded file to /home/div/stanza_resources/resources.json\n",
      "2026-02-21 14:56:14 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-02-21 14:56:15 INFO: Loading these models for language: en (English):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2026-02-21 14:56:15 INFO: Using device: cpu\n",
      "2026-02-21 14:56:15 INFO: Loading: tokenize\n",
      "2026-02-21 14:56:15 INFO: Loading: mwt\n",
      "2026-02-21 14:56:15 INFO: Loading: pos\n",
      "2026-02-21 14:56:18 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download English model\n",
    "stanza.download('en')\n",
    "\n",
    "# Initialize pipeline\n",
    "nlp_stanza = stanza.Pipeline(\n",
    "    lang='en',\n",
    "    processors='tokenize,pos',\n",
    "    use_gpu=False\n",
    ")\n",
    "\n",
    "stanza_tags = []\n",
    "\n",
    "for doc_text in text:\n",
    "    doc = nlp_stanza(doc_text)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            stanza_tags.append((word.text, word.upos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be5fdd3a-0e18-470b-b3f6-e3ca475e840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "spacy_tags = []\n",
    "\n",
    "for doc_text in text:\n",
    "    doc = nlp_spacy(doc_text)\n",
    "    for token in doc:\n",
    "        spacy_tags.append((token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eafce20-0a74-4e3c-8bbe-9b3b8ab8b9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanza sample: [('The', 'DET'), ('Fulton', 'PROPN'), ('County', 'PROPN'), ('Grand', 'ADJ'), ('Jury', 'PROPN'), ('said', 'VERB'), ('Friday', 'PROPN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP')]\n",
      "spaCy sample: [('The', 'DET'), ('Fulton', 'PROPN'), ('County', 'PROPN'), ('Grand', 'PROPN'), ('Jury', 'PROPN'), ('said', 'VERB'), ('Friday', 'PROPN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Stanza sample:\", stanza_tags[:10])\n",
    "print(\"spaCy sample:\", spacy_tags[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c8ea1-5078-4d26-bff5-52dcd64424ef",
   "metadata": {},
   "source": [
    "### 2. Write a python program for lemmatization using spaCy and stanza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8d94a27-b969-4faf-9e30-075483d1b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/div/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "\n",
    "# Use a reasonably sized subset\n",
    "sentences = brown.sents()[:2]\n",
    "text = [\" \".join(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "813ed80c-b026-4cc5-b328-0a4101401c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "spacy_lemmas = []\n",
    "\n",
    "for doc_text in text:\n",
    "    doc = nlp_spacy(doc_text)\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            spacy_lemmas.append((token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61d1f3f1-236a-432e-a063-ddcaa5f5dd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 439kB [00:00, 22.3MB/s]                                                                 \n",
      "2026-02-21 14:59:55 INFO: Downloaded file to /home/div/stanza_resources/resources.json\n",
      "2026-02-21 14:59:55 INFO: Downloading default packages for language: en (English) ...\n",
      "2026-02-21 14:59:56 INFO: File exists: /home/div/stanza_resources/en/default.zip\n",
      "2026-02-21 15:00:07 INFO: Finished downloading models and saved to /home/div/stanza_resources\n",
      "2026-02-21 15:00:07 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 439kB [00:00, 15.7MB/s]                                                                 \n",
      "2026-02-21 15:00:07 INFO: Downloaded file to /home/div/stanza_resources/resources.json\n",
      "2026-02-21 15:00:07 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-02-21 15:00:08 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2026-02-21 15:00:08 INFO: Using device: cpu\n",
      "2026-02-21 15:00:08 INFO: Loading: tokenize\n",
      "2026-02-21 15:00:08 INFO: Loading: mwt\n",
      "2026-02-21 15:00:08 INFO: Loading: pos\n",
      "2026-02-21 15:00:11 INFO: Loading: lemma\n",
      "2026-02-21 15:00:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download('en')\n",
    "\n",
    "nlp_stanza = stanza.Pipeline(\n",
    "    lang='en',\n",
    "    processors='tokenize,pos,lemma',\n",
    "    use_gpu=False\n",
    ")\n",
    "\n",
    "stanza_lemmas = []\n",
    "\n",
    "for doc_text in text:\n",
    "    doc = nlp_stanza(doc_text)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.text.isalpha():\n",
    "                stanza_lemmas.append((word.text, word.lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "821e3caf-b865-40e0-b0d7-05aa71255f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy lemmas: [('The', 'the'), ('Fulton', 'Fulton'), ('County', 'County'), ('Grand', 'Grand'), ('Jury', 'Jury'), ('said', 'say'), ('Friday', 'Friday'), ('an', 'an'), ('investigation', 'investigation'), ('of', 'of')]\n",
      "Stanza lemmas: [('The', 'the'), ('Fulton', 'Fulton'), ('County', 'County'), ('Grand', 'Grand'), ('Jury', 'Jury'), ('said', 'say'), ('Friday', 'Friday'), ('an', 'a'), ('investigation', 'investigation'), ('of', 'of')]\n"
     ]
    }
   ],
   "source": [
    "print(\"spaCy lemmas:\", spacy_lemmas[:10])\n",
    "print(\"Stanza lemmas:\", stanza_lemmas[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
