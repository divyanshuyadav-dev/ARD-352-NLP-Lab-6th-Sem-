{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0008bbcd-315e-411c-8353-b362c69aac42",
   "metadata": {},
   "source": [
    "# Experiment 1: Text Processing Using NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3bfaf-89ac-4f26-b25b-67923e436c1e",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "To preprocess raw text by applying tokenization, stop word removal, and stemming using Python and the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b850be-0233-41e0-8c73-d42fedf27c2e",
   "metadata": {},
   "source": [
    "## Tools Used\n",
    "\n",
    "- Python 3\n",
    "- NLTK (Natural Language Toolkit)\n",
    "- Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f0035-b258-4d1e-b34d-f820b58d46ac",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "Text preprocessing is a fundamental step in Natural Language Processing (NLP) that converts unstructured text into a structured and machine-understandable form.\n",
    "It helps reduce noise and dimensionality before applying higher-level NLP tasks such as text classification, sentiment analysis, and information retrieval.\n",
    "\n",
    "The main preprocessing steps involved are:\n",
    "\n",
    "- **Tokenization:** Breaking text into sentences and words.\n",
    "\n",
    "- **Stopword Removal**: Eliminating commonly occurring words that carry little semantic meaning.\n",
    "\n",
    "- **Stemming**: Reducing words to their root form using rule-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d1849-3c30-436e-87a2-f5a6f2e427e8",
   "metadata": {},
   "source": [
    "## Code\n",
    "### Step 1: Import Libraries and Download Required Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf00ffd-dc66-4ddb-854a-4ea0166cd4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/div/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/div/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab495aed-04ad-40ff-b9cc-276c99d92767",
   "metadata": {},
   "source": [
    "### Step 2: Input Text and Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4894c9f7-d091-4269-85dc-03b0d0932244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum \n",
      "lowered:nltk is a leading platform for building python programs to work with human language data. it provides easy-to-use interfaces to over 50 corpora and lexical resources such as wordnet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength nlp libraries, and an active discussion forum\n"
     ]
    }
   ],
   "source": [
    "og_text = \"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum\"\n",
    "\n",
    "# normalise to lowercase, important for stopword removal\n",
    "lower_text = og_text.lower()\n",
    "print(f\"original:{og_text} \\nlowered:{lower_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b5eaf-7b7e-4cfd-bc2d-8b691122f778",
   "metadata": {},
   "source": [
    "### Step 3: Sentence and Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10348087-3778-46f8-b76a-45addfb070c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized words:['nltk', 'is', 'a', 'leading', 'platform', 'for', 'building', 'python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'it', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'wordnet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'nlp', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum']\n",
      "tokenized sentences:['nltk is a leading platform for building python programs to work with human language data.', 'it provides easy-to-use interfaces to over 50 corpora and lexical resources such as wordnet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength nlp libraries, and an active discussion forum']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(lower_text)\n",
    "sentences = sent_tokenize(lower_text)\n",
    "print(f\"tokenized words:{words}\")\n",
    "print(f\"tokenized sentences:{sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3aad5-9b28-48b4-8d40-278127ab1ede",
   "metadata": {},
   "source": [
    "### Step 4: Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342faa83-2eb2-4b33-ac05-b30e36d768d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with stop-words:['nltk', 'is', 'a', 'leading', 'platform', 'for', 'building', 'python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'it', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'wordnet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'nlp', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum']\n",
      "without stop-words:['nltk', 'leading', 'platform', 'building', 'python', 'programs', 'work', 'human', 'language', 'data', '.', 'provides', 'easy-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', 'wordnet', ',', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'semantic', 'reasoning', ',', 'wrappers', 'industrial-strength', 'nlp', 'libraries', ',', 'active', 'discussion', 'forum']\n"
     ]
    }
   ],
   "source": [
    "stp_words = set(stopwords.words('english'))\n",
    "clean_words = [word for word in words if word not in stp_words]\n",
    "print(f\"with stop-words:{words}\")\n",
    "print(f\"without stop-words:{clean_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0921b478-4044-4540-bf4c-2c9103ac9864",
   "metadata": {},
   "source": [
    "### Step 5: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ede450-9b17-422b-b740-48af8db84dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wihtout stemming:['nltk', 'leading', 'platform', 'building', 'python', 'programs', 'work', 'human', 'language', 'data', '.', 'provides', 'easy-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', 'wordnet', ',', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'semantic', 'reasoning', ',', 'wrappers', 'industrial-strength', 'nlp', 'libraries', ',', 'active', 'discussion', 'forum']\n",
      "with stemming:['nltk', 'lead', 'platform', 'build', 'python', 'program', 'work', 'human', 'languag', 'data', '.', 'provid', 'easy-to-us', 'interfac', '50', 'corpora', 'lexic', 'resourc', 'wordnet', ',', 'along', 'suit', 'text', 'process', 'librari', 'classif', ',', 'token', ',', 'stem', ',', 'tag', ',', 'pars', ',', 'semant', 'reason', ',', 'wrapper', 'industrial-strength', 'nlp', 'librari', ',', 'activ', 'discuss', 'forum']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in clean_words]\n",
    "print(f\"wihtout stemming:{clean_words}\")\n",
    "print(f\"with stemming:{stemmed_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
